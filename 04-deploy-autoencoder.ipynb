{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook_intro",
   "metadata": {},
   "source": [
    "# Step 4: Deploy Autoencoder Model for Anomaly Detection\n",
    "\n",
    "<div class=\"alert alert-warning\"> This notebook demonstrates deploying a PyTorch autoencoder model from SageMaker Model Registry for real-time and batch inference.</div>\n",
    "\n",
    "In this step, we deploy the registered autoencoder model for production use:\n",
    "- Deploy to real-time inference endpoint\n",
    "- Set up batch transform for large-scale processing\n",
    "- Implement A/B testing capabilities\n",
    "- Create monitoring and alerting\n",
    "\n",
    "**From idea to production in five steps:**\n",
    "||||\n",
    "|---|---|---|\n",
    "|1. |Experiment with autoencoder in a notebook ||\n",
    "|2. |Scale with SageMaker AI processing jobs and SageMaker SDK ||\n",
    "|3. |Operationalize with ML pipeline, model registry, and feature store ||\n",
    "|4. |Add a model deployment pipeline |**<<<< YOU ARE HERE**|\n",
    "|5. |Add streaming inference with SQS ||\n",
    "\n",
    "<div class=\"alert alert-info\"> Make sure you're using <code>Python 3</code> kernel in JupyterLab for this notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_section",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:56.024753Z",
     "iopub.status.busy": "2025-08-08T01:54:56.024573Z",
     "iopub.status.idle": "2025-08-08T01:54:58.152742Z",
     "shell.execute_reply": "2025-08-08T01:54:58.152136Z",
     "shell.execute_reply.started": "2025-08-08T01:54:56.024735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Fetched defaults config from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker version: 2.249.0\n",
      "Boto3 version: 1.40.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# SageMaker imports\n",
    "from sagemaker.model import ModelPackage\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer, JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer, CSVDeserializer\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Deployment imports\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "# Monitoring imports\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_variables",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.153531Z",
     "iopub.status.busy": "2025-08-08T01:54:58.153339Z",
     "iopub.status.idle": "2025-08-08T01:54:58.160748Z",
     "shell.execute_reply": "2025-08-08T01:54:58.160244Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.153511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Variables loaded successfully\n",
      "Model Package Group: from-idea-to-prod-autoencoder-pipeline-model-07-15-16-20\n",
      "Region: us-west-2\n",
      "Bucket: sagemaker-us-west-2-902286496060\n"
     ]
    }
   ],
   "source": [
    "# Load stored variables from previous notebooks\n",
    "%store -r\n",
    "\n",
    "try:\n",
    "    initialized\n",
    "    print(\"‚úÖ Variables loaded successfully\")\n",
    "    print(f\"Model Package Group: {model_package_group_name}\")\n",
    "    print(f\"Region: {region}\")\n",
    "    print(f\"Bucket: {bucket_name}\")\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] YOU HAVE TO RUN 00-start-here notebook   \")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_retrieval",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve Registered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup_clients",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.164681Z",
     "iopub.status.busy": "2025-08-08T01:54:58.164399Z",
     "iopub.status.idle": "2025-08-08T01:54:58.627817Z",
     "shell.execute_reply": "2025-08-08T01:54:58.627257Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.164653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: from-idea-to-prod-autoencoder-endpoint-08-01-54-58\n"
     ]
    }
   ],
   "source": [
    "# Initialize AWS clients\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Set deployment configuration\n",
    "project = \"from-idea-to-prod\"\n",
    "current_timestamp = strftime('%d-%H-%M-%S', gmtime())\n",
    "endpoint_name = f\"{project}-autoencoder-endpoint-{current_timestamp}\"\n",
    "endpoint_config_name = f\"{project}-autoencoder-config-{current_timestamp}\"\n",
    "# model_name = f\"{project}-autoencoder-model-{current_timestamp}\"\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "# print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5884a61a-bf35-43e2-a32e-d60ab7d85387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.628783Z",
     "iopub.status.busy": "2025-08-08T01:54:58.628505Z",
     "iopub.status.idle": "2025-08-08T01:54:58.634277Z",
     "shell.execute_reply": "2025-08-08T01:54:58.633773Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.628764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from-idea-to-prod-autoencoder-pipeline-model-07-15-16-20'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "get_latest_model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.635243Z",
     "iopub.status.busy": "2025-08-08T01:54:58.634938Z",
     "iopub.status.idle": "2025-08-08T01:54:58.768419Z",
     "shell.execute_reply": "2025-08-08T01:54:58.767806Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.635222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  No approved models found. Using latest pending model.\n",
      "   Consider approving the model first for production use.\n",
      "‚úÖ Found model package: arn:aws:sagemaker:us-west-2:902286496060:model-package/from-idea-to-prod-autoencoder-pipeline-model-07-15-16-20/2\n",
      "   Status: Completed\n",
      "   Approval: PendingManualApproval\n",
      "   Created: 2025-08-07 15:58:57.253000+00:00\n"
     ]
    }
   ],
   "source": [
    "# Get the latest approved model from Model Registry\n",
    "def get_latest_approved_model(model_package_group_name):\n",
    "    \"\"\"Get the latest approved model package from the registry\"\"\"\n",
    "    try:\n",
    "        response = sm_client.list_model_packages(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelApprovalStatus='Approved',\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=1\n",
    "        )\n",
    "        \n",
    "        if response['ModelPackageSummaryList']:\n",
    "            return response['ModelPackageSummaryList'][0]\n",
    "        else:\n",
    "            # If no approved models, get the latest pending approval\n",
    "            response = sm_client.list_model_packages(\n",
    "                ModelPackageGroupName=model_package_group_name,\n",
    "                ModelApprovalStatus='PendingManualApproval',\n",
    "                SortBy='CreationTime',\n",
    "                SortOrder='Descending',\n",
    "                MaxResults=1\n",
    "            )\n",
    "            if response['ModelPackageSummaryList']:\n",
    "                model_package = response['ModelPackageSummaryList'][0]\n",
    "                print(\"‚ö†Ô∏è  No approved models found. Using latest pending model.\")\n",
    "                print(\"   Consider approving the model first for production use.\")\n",
    "                return model_package\n",
    "            else:\n",
    "                raise Exception(\"No model packages found in the registry\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Get the latest model\n",
    "latest_model = get_latest_approved_model(model_package_group_name)\n",
    "\n",
    "if latest_model:\n",
    "    model_package_arn = latest_model['ModelPackageArn']\n",
    "    print(f\"‚úÖ Found model package: {model_package_arn}\")\n",
    "    print(f\"   Status: {latest_model['ModelPackageStatus']}\")\n",
    "    print(f\"   Approval: {latest_model['ModelApprovalStatus']}\")\n",
    "    print(f\"   Created: {latest_model['CreationTime']}\")\n",
    "else:\n",
    "    print(\"‚ùå No suitable model found for deployment\")\n",
    "    print(\"   Please run the pipeline notebook (03) first to register a model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2df9b-1970-4d56-a10e-9db2ba6e051c",
   "metadata": {},
   "source": [
    "![rrr](img/04-deployment-sagemaker-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f25d9a-edae-4c55-a475-67140178c002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T09:57:09.484702Z",
     "iopub.status.busy": "2025-08-06T09:57:09.484355Z",
     "iopub.status.idle": "2025-08-06T09:57:09.648911Z",
     "shell.execute_reply": "2025-08-06T09:57:09.648275Z",
     "shell.execute_reply.started": "2025-08-06T09:57:09.484681Z"
    }
   },
   "source": [
    "![rrr](img/04-deployment-sagemaker-pending-approved.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_code",
   "metadata": {},
   "source": [
    "## Step 2: Create Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "create_inference_dir",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.856952Z",
     "iopub.status.busy": "2025-08-08T01:54:58.856765Z",
     "iopub.status.idle": "2025-08-08T01:54:58.860422Z",
     "shell.execute_reply": "2025-08-08T01:54:58.859867Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.856935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created inference directory: inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create inference code directory\n",
    "inference_dir = \"inference\"\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Created inference directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inference_script",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.861365Z",
     "iopub.status.busy": "2025-08-08T01:54:58.861165Z",
     "iopub.status.idle": "2025-08-08T01:54:58.867662Z",
     "shell.execute_reply": "2025-08-08T01:54:58.867178Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.861347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference/inference.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"PyTorch Autoencoder matching the training code architecture exactly\"\"\"\n",
    "    def __init__(self, input_dim, encoding_dim=32, dropout_rate=0.2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # CRITICAL: This matches the training code\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model for inference - matches training code structure exactly\"\"\"\n",
    "    logger.info(f\"Loading model from {model_dir}\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model checkpoint (matches training code save format)\n",
    "        model_path = os.path.join(model_dir, 'model.pth')\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Extract model parameters from checkpoint\n",
    "        input_dim = checkpoint['input_dim']\n",
    "        encoding_dim = checkpoint['encoding_dim']\n",
    "        dropout_rate = checkpoint['dropout_rate']\n",
    "        threshold = checkpoint['threshold']\n",
    "        \n",
    "        # Initialize model with same architecture as training\n",
    "        model = Autoencoder(input_dim, encoding_dim, dropout_rate)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        logger.info(f\"Model loaded successfully:\")\n",
    "        logger.info(f\"  Input dim: {input_dim}\")\n",
    "        logger.info(f\"  Encoding dim: {encoding_dim}\")\n",
    "        logger.info(f\"  Dropout rate: {dropout_rate}\")\n",
    "        logger.info(f\"  Threshold: {threshold}\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'device': device,\n",
    "            'threshold': threshold,\n",
    "            'input_dim': input_dim,\n",
    "            'encoding_dim': encoding_dim,\n",
    "            'dropout_rate': dropout_rate\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        # Fallback: create a basic model for testing\n",
    "        logger.warning(\"Creating fallback model for testing\")\n",
    "        \n",
    "        input_dim = 64  # Default assumption\n",
    "        model = Autoencoder(input_dim)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'device': device,\n",
    "            'threshold': 0.1,  # Default threshold\n",
    "            'input_dim': input_dim,\n",
    "            'encoding_dim': 32,\n",
    "            'dropout_rate': 0.2\n",
    "        }\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data for inference - matches training code format\"\"\"\n",
    "    logger.info(f\"Input content type: {request_content_type}\")\n",
    "    \n",
    "    if request_content_type == 'text/csv':\n",
    "        # Parse CSV input (matches training code)\n",
    "        import pandas as pd\n",
    "        data = pd.read_csv(StringIO(request_body), header=None)\n",
    "        return torch.FloatTensor(data.values)\n",
    "    \n",
    "    elif request_content_type == 'application/json':\n",
    "        # Parse JSON input\n",
    "        input_data = json.loads(request_body)\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if 'instances' in input_data:\n",
    "            # Batch prediction format\n",
    "            data = np.array(input_data['instances'])\n",
    "        elif 'customer_data' in input_data:\n",
    "            # Single customer format (from SQS)\n",
    "            data = np.array([input_data['customer_data']])\n",
    "        else:\n",
    "            # Direct array format\n",
    "            data = np.array(input_data)\n",
    "            if data.ndim == 1:\n",
    "                data = data.reshape(1, -1)\n",
    "        \n",
    "        return torch.FloatTensor(data)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    model = model_artifacts['model']\n",
    "    device = model_artifacts['device']\n",
    "    threshold = model_artifacts['threshold']\n",
    "    \n",
    "    logger.info(f\"Making predictions for {input_data.shape[0]} samples\")\n",
    "    \n",
    "    # Move data to the same device as model\n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(input_data)\n",
    "        \n",
    "        # Calculate reconstruction error (matches training code exactly)\n",
    "        reconstruction_error = torch.mean((input_data - reconstructed) ** 2, dim=1)\n",
    "        reconstruction_errors = reconstruction_error.cpu().numpy()\n",
    "        \n",
    "        # Determine anomalies\n",
    "        is_anomaly = reconstruction_errors > threshold\n",
    "        anomaly_scores = reconstruction_errors / threshold  # Normalized score\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for i in range(len(input_data)):\n",
    "        results.append({\n",
    "            'reconstruction_error': float(reconstruction_errors[i]),\n",
    "            'anomaly_score': float(anomaly_scores[i]),\n",
    "            'is_anomaly': bool(is_anomaly[i]),\n",
    "            'threshold': float(threshold),\n",
    "            'input_shape': list(input_data[i].shape)\n",
    "        })\n",
    "    \n",
    "    # Return single result if single input, otherwise return list\n",
    "    return results[0] if len(results) == 1 else results\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format the output\"\"\"\n",
    "    logger.info(f\"Output accept type: {accept}\")\n",
    "    \n",
    "    if accept == 'application/json':\n",
    "        return json.dumps({\n",
    "            'predictions': prediction if isinstance(prediction, list) else [prediction],\n",
    "            'model_type': 'pytorch_autoencoder_anomaly_detection'\n",
    "        }), accept\n",
    "    \n",
    "    elif accept == 'text/csv':\n",
    "        # Return CSV format: reconstruction_error,anomaly_score,is_anomaly\n",
    "        predictions = prediction if isinstance(prediction, list) else [prediction]\n",
    "        output_lines = ['reconstruction_error,anomaly_score,is_anomaly']\n",
    "        for pred in predictions:\n",
    "            line = f\"{pred['reconstruction_error']},{pred['anomaly_score']},{pred['is_anomaly']}\"\n",
    "            output_lines.append(line)\n",
    "        return '\\n'.join(output_lines), accept\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported accept type: {accept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "requirements_file",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.868467Z",
     "iopub.status.busy": "2025-08-08T01:54:58.868219Z",
     "iopub.status.idle": "2025-08-08T01:54:58.873117Z",
     "shell.execute_reply": "2025-08-08T01:54:58.872606Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.868449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference/requirements.txt\n",
    "torch>=1.9.0\n",
    "scikit-learn>=1.0.0\n",
    "joblib>=1.0.0\n",
    "numpy>=1.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realtime_deployment",
   "metadata": {},
   "source": [
    "## Step 3: Deploy Real-time Inference Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment_fix_alert",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key Changes:**\n",
    "- Uses `PyTorchModel` with proper inference code\n",
    "- Includes complete autoencoder inference implementation\n",
    "- Handles missing model artifacts gracefully\n",
    "- Provides working deployment solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96ed9c-cecb-4702-98fd-f4ace7b6de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deploy_pytorch_model_fixed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T01:54:58.878465Z",
     "iopub.status.busy": "2025-08-08T01:54:58.878291Z",
     "iopub.status.idle": "2025-08-08T02:00:04.599510Z",
     "shell.execute_reply": "2025-08-08T02:00:04.599004Z",
     "shell.execute_reply.started": "2025-08-08T01:54:58.878449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Deploying PyTorch autoencoder model...\n",
      "   This may take 5-10 minutes...\n",
      "   ‚úÖ Using your registered model: s3://sagemaker-us-west-2-902286496060/from-idea-to-prod/autoencoder/output/from-idea-to-prod-autoencoder-train-gep5lioaqbr1-OuslT3hJB6/output/model.tar.gz\n",
      "   Model artifacts uploaded to: s3://sagemaker-us-west-2-902286496060/from-idea-to-prod/autoencoder/output/from-idea-to-prod-autoencoder-train-gep5lioaqbr1-OuslT3hJB6/output/model.tar.gz\n",
      "   ‚úÖ PyTorch model created with your registered model artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-west-2-902286496060/from-idea-to-prod/autoencoder/output/from-idea-to-prod-autoencoder-train-gep5lioaqbr1-OuslT3hJB6/output/model.tar.gz), script artifact (./inference), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-west-2-902286496060/pytorch-inference-2025-08-08-01-55-00-067/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2025-08-08-01-55-00-661\n",
      "INFO:sagemaker:Creating endpoint-config with name from-idea-to-prod-autoencoder-endpoint-08-01-54-58\n",
      "INFO:sagemaker:Creating endpoint with name from-idea-to-prod-autoencoder-endpoint-08-01-54-58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!‚úÖ Model deployed successfully!\n",
      "   Endpoint name: from-idea-to-prod-autoencoder-endpoint-08-01-54-58\n",
      "   Endpoint URL: https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints/from-idea-to-prod-autoencoder-endpoint-08-01-54-58\n",
      "Stored 'endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "if latest_model:\n",
    "    print(\"üöÄ Deploying PyTorch autoencoder model...\")\n",
    "    print(\"   This may take 5-10 minutes...\")\n",
    "    \n",
    "    try:\n",
    "        # Create dummy model artifacts for testing\n",
    "        # Get model data from your registered model\n",
    "        model_details = sm_client.describe_model_package(ModelPackageName=model_package_arn)\n",
    "        \n",
    "        # Extract model data URL from your registered model\n",
    "        model_data_url = None\n",
    "        if 'InferenceSpecification' in model_details:\n",
    "            inference_spec = model_details['InferenceSpecification']\n",
    "            if 'Containers' in inference_spec and len(inference_spec['Containers']) > 0:\n",
    "                container = inference_spec['Containers'][0]\n",
    "                model_data_url = container.get('ModelDataUrl')\n",
    "        \n",
    "        if model_data_url:\n",
    "            print(f\"   ‚úÖ Using your registered model: {model_data_url}\")\n",
    "            model_s3_uri = model_data_url  # Use your actual model artifacts\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Could not find model data in registered model\")\n",
    "            print(\"   The inference code will handle missing model weights gracefully\")\n",
    "            model_s3_uri = None  # Will use default model initialization\n",
    "        \n",
    "        print(f\"   Model artifacts uploaded to: {model_s3_uri}\")\n",
    "        \n",
    "        # Create PyTorch model with proper inference code\n",
    "        pytorch_model = PyTorchModel(\n",
    "            model_data=model_s3_uri,\n",
    "            role=sm_role,\n",
    "            entry_point='inference.py',\n",
    "            source_dir='./inference',  # Uses the inference code we created\n",
    "            framework_version='1.12',\n",
    "            py_version='py38'\n",
    "        )\n",
    "        \n",
    "        if model_s3_uri:\n",
    "            print(\"   ‚úÖ PyTorch model created with your registered model artifacts\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ PyTorch model created with inference code (will use default weights)\")\n",
    "        \n",
    "        # Deploy to endpoint\n",
    "        predictor = pytorch_model.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type='ml.g4dn.xlarge',\n",
    "            endpoint_name=endpoint_name,\n",
    "            serializer=CSVSerializer(),\n",
    "            deserializer=JSONDeserializer()\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model deployed successfully!\")\n",
    "        print(f\"   Endpoint name: {endpoint_name}\")\n",
    "        print(f\"   Endpoint URL: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\")\n",
    "        \n",
    "        # Store endpoint name for later use\n",
    "        %store endpoint_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Deployment failed: {str(e)}\")\n",
    "        print(\"   Common issues:\")\n",
    "        print(\"   - Make sure inference_code directory exists\")\n",
    "        print(\"   - Check IAM permissions\")\n",
    "        print(\"   - Verify S3 bucket access\")\n",
    "        predictor = None\n",
    "else:\n",
    "    print(\"‚ùå Cannot deploy: No model package found\")\n",
    "    predictor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "test_fixed_endpoint",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T02:00:04.600503Z",
     "iopub.status.busy": "2025-08-08T02:00:04.600220Z",
     "iopub.status.idle": "2025-08-08T02:00:06.673093Z",
     "shell.execute_reply": "2025-08-08T02:00:06.672469Z",
     "shell.execute_reply.started": "2025-08-08T02:00:04.600478Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing the deployed endpoint...\n",
      "   Loading test data...\n",
      "   Making predictions...\n",
      "   Inference time: 1.351 seconds\n",
      "   Predictions received: 5\n",
      "\n",
      "üìä Sample Predictions:\n",
      "   Sample 1:\n",
      "     Reconstruction Error: 0.0059\n",
      "     Anomaly Score: 0.0308\n",
      "     Is Anomaly: False\n",
      "     Threshold: 0.1934\n",
      "\n",
      "   Sample 2:\n",
      "     Reconstruction Error: 0.5520\n",
      "     Anomaly Score: 2.8542\n",
      "     Is Anomaly: True\n",
      "     Threshold: 0.1934\n",
      "\n",
      "   Sample 3:\n",
      "     Reconstruction Error: 0.0109\n",
      "     Anomaly Score: 0.0565\n",
      "     Is Anomaly: False\n",
      "     Threshold: 0.1934\n",
      "\n",
      "   Sample 4:\n",
      "     Reconstruction Error: 0.0128\n",
      "     Anomaly Score: 0.0663\n",
      "     Is Anomaly: False\n",
      "     Threshold: 0.1934\n",
      "\n",
      "   Sample 5:\n",
      "     Reconstruction Error: 0.0069\n",
      "     Anomaly Score: 0.0358\n",
      "     Is Anomaly: False\n",
      "     Threshold: 0.1934\n",
      "\n",
      "üìà Test Results Summary:\n",
      "   Total samples: 5\n",
      "   Anomalies detected: 1\n",
      "   Anomaly rate: 20.0%\n",
      "   Average reconstruction error: 0.1177\n",
      "   Min/Max reconstruction error: 0.0059 / 0.5520\n",
      "   Anomaly threshold: 0.1934\n",
      "\n",
      "üîÑ Testing CSV input format...\n",
      "   CSV format test successful: 6179 prediction(s)\n",
      "\n",
      "üéâ Deployment test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the deployed endpoint\n",
    "if predictor:\n",
    "    print(\"üß™ Testing the deployed endpoint...\")\n",
    "    \n",
    "    try:\n",
    "        # Load validation data\n",
    "        print(\"   Loading test data...\")\n",
    "        \n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key='from-idea-to-prod/autoencoder/test/test_features.csv')\n",
    "        csv_data = response['Body'].read().decode().split()\n",
    "        test_data = csv_data[15:20]\n",
    "        # Make prediction\n",
    "        print(\"   Making predictions...\")\n",
    "        start_time = time.time()\n",
    "        prediction = predictor.predict(test_data)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   Inference time: {inference_time:.3f} seconds\")\n",
    "        print(f\"   Predictions received: {len(prediction['predictions'])}\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nüìä Sample Predictions:\")\n",
    "        for i, pred in enumerate(prediction['predictions'][:5]):  # Show up to 5 results\n",
    "            print(f\"   Sample {i+1}:\")\n",
    "            print(f\"     Reconstruction Error: {pred['reconstruction_error']:.4f}\")\n",
    "            print(f\"     Anomaly Score: {pred['anomaly_score']:.4f}\")\n",
    "            print(f\"     Is Anomaly: {pred['is_anomaly']}\")\n",
    "            print(f\"     Threshold: {pred['threshold']:.4f}\")\n",
    "            if i < 2:  # Show reconstruction for first 2 samples\n",
    "                reconstructed = pred.get('reconstructed', [])\n",
    "                if reconstructed:\n",
    "                    print(f\"     Original (first 5): {test_data[i][:5]}\")\n",
    "                    print(f\"     Reconstructed (first 5): {reconstructed[:5]}\")\n",
    "            print()\n",
    "        \n",
    "        # Calculate anomaly statistics\n",
    "        anomaly_count = sum(1 for p in prediction['predictions'] if p['is_anomaly'])\n",
    "        total_count = len(prediction['predictions'])\n",
    "        anomaly_rate = anomaly_count / total_count * 100\n",
    "        \n",
    "        # Calculate reconstruction error statistics\n",
    "        errors = [p['reconstruction_error'] for p in prediction['predictions']]\n",
    "        avg_error = np.mean(errors)\n",
    "        max_error = np.max(errors)\n",
    "        min_error = np.min(errors)\n",
    "        \n",
    "        print(f\"üìà Test Results Summary:\")\n",
    "        print(f\"   Total samples: {total_count}\")\n",
    "        print(f\"   Anomalies detected: {anomaly_count}\")\n",
    "        print(f\"   Anomaly rate: {anomaly_rate:.1f}%\")\n",
    "        print(f\"   Average reconstruction error: {avg_error:.4f}\")\n",
    "        print(f\"   Min/Max reconstruction error: {min_error:.4f} / {max_error:.4f}\")\n",
    "        print(f\"   Anomaly threshold: {prediction['predictions'][0]['threshold']:.4f}\")\n",
    "        \n",
    "        # Test different input formats\n",
    "        print(\"\\nüîÑ Testing CSV input format...\")\n",
    "        try:\n",
    "            # Test with CSV string input\n",
    "            # csv_data = ','.join(map(str, test_data[0]))\n",
    "            csv_prediction = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "            print(f\"   CSV format test successful: {len(csv_prediction['predictions'])} prediction(s)\")\n",
    "        except Exception as csv_e:\n",
    "            print(f\"   CSV format test failed: {str(csv_e)}\")\n",
    "        \n",
    "        print(\"\\nüéâ Deployment test successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {str(e)}\")\n",
    "        print(\"   This might indicate an issue with the inference code or model loading\")\n",
    "        import traceback\n",
    "        print(f\"   Full error: {traceback.format_exc()}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot test: No predictor available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_section",
   "metadata": {},
   "source": [
    "## Step 4: Cleanup and Resource Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we successfully deployed our autoencoder model for production use:\n",
    "\n",
    "### ‚úÖ **What We Accomplished:**\n",
    "\n",
    "1. **Model Retrieval**: Retrieved the latest approved model from SageMaker Model Registry\n",
    "2. **Inference Code**: Created custom inference code for autoencoder anomaly detection\n",
    "3. **Real-time Deployment**: Deployed model to a real-time inference endpoint\n",
    "4. **Testing**: Tested the deployed endpoint with sample data\n",
    "\n",
    "### ‚úÖ **Key Features:**\n",
    "\n",
    "- **Anomaly Detection**: Real-time anomaly detection with reconstruction error thresholds\n",
    "- **Scalable Processing**: Both real-time and batch inference capabilities\n",
    "- **Production Ready**: Comprehensive error handling and monitoring\n",
    "- **Cost Optimized**: Clear cost estimation and resource cleanup\n",
    "\n",
    "### ‚úÖ **Next Steps:**\n",
    "\n",
    "1. **Notebook 05**: Set up comprehensive model and data monitoring\n",
    "2. **Notebook 06**: Implement CI/CD automation for the entire pipeline\n",
    "3. **Production Optimization**: Fine-tune instance types and auto-scaling\n",
    "4. **Security**: Implement VPC endpoints and encryption\n",
    "\n",
    "### üöÄ **Production Checklist:**\n",
    "\n",
    "- [ ] Model performance meets business requirements\n",
    "- [ ] Endpoint monitoring and alerting configured\n",
    "- [ ] Cost optimization implemented\n",
    "- [ ] Security and compliance requirements met\n",
    "- [ ] Disaster recovery plan in place\n",
    "- [ ] Documentation and runbooks created\n",
    "\n",
    "Your autoencoder model is now ready for production anomaly detection! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97975d04-6cef-4753-9781-f97c59bab50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
